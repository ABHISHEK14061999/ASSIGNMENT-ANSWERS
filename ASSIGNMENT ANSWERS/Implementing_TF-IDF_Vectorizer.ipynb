{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "strings = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(strings, unique_words): #func for idf()\n",
    "    idf={}\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for j in strings:\n",
    "            if i in j.split(\" \"):\n",
    "                count +=1\n",
    "        idf[i]=1+math.log((1+len(strings))/float(1+count))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(strings):              #func for fit()\n",
    "    req_of_word=[]\n",
    "    words=set()    #creating an empty set \n",
    "    count={}\n",
    "    if isinstance(strings,(list)): \n",
    "        for row in strings:\n",
    "            split_text=row.split(\" \")\n",
    "            for word in split_text:\n",
    "                if word in count:\n",
    "                    count[word]=count[word]+1        #count words in corpus\n",
    "                else:\n",
    "                    count[word]=1    \n",
    "        count=dict(sorted(count.items()))       #sort the dict\n",
    "        for i in strings:\n",
    "            for j in i.split(\" \"):    #split the strings acc to spaces\n",
    "                if len(j)<2:\n",
    "                    continue\n",
    "                words.add(j)             #append word length greater than 2 to the set\n",
    "        words=sorted(list(words))      #convert set into list and sort them\n",
    "        vocab={}                      #empty dictionary\n",
    "        for val,key in enumerate(words):\n",
    "            vocab[key]=val            #add splitted word as key and index as value in dict\n",
    "              \n",
    "            \n",
    "        unique_idf_val=idf(strings,vocab)\n",
    "        return vocab,unique_idf_val    #return vocabulary and unique idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, idf_val=fit(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1.916290731874155, 'document': 1.2231435513142097, 'first': 1.5108256237659907, 'is': 1.0, 'one': 1.916290731874155, 'second': 1.916290731874155, 'the': 1.0, 'third': 1.916290731874155, 'this': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print((idf_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "idf_v = np.array(list(idf_val.values())) \n",
    "idf_value_of_unique_words = list(idf_v) \n",
    "print(np.array(idf_value_of_unique_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import normalize #import Normalizer package from sklearn for L2 norm\n",
    "import numpy as np\n",
    "from collections import Counter #importing counter for count of words in corpus\n",
    "from scipy.sparse import csr_matrix # convert list input to csr sparse matrix with ease\n",
    "def transform(strings,vocab,idf_val):  #transform function takes corpus and (words,index) as input\n",
    "    sparse_mat=csr_matrix((len(strings),len(vocab)), dtype='float64')\n",
    "    for row_in_string in range(0,len(strings)):\n",
    "        word_in_sent=Counter(strings[row_in_string].split())\n",
    "        for i in strings[row_in_string].split():\n",
    "            if i in list(vocab.keys()):\n",
    "                tfidf_val=(word_in_sent[i]/len(strings[row_in_string].split()))*(idf_val[i])\n",
    "                sparse_mat[row_in_string,vocab[i]]=tfidf_val\n",
    "    final_output =normalize(sparse_mat, norm='l2', axis=1, copy=True, return_norm=False) #normalize\n",
    "    return final_output  #returning normalize sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek V\\Anaconda2020\\lib\\site-packages\\scipy\\sparse\\_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "final_output = transform(strings , vocab, idf_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(final_output)  #sparse matrix matched with instruction notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "print(final_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(final_output[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sklearn implimentation of idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(strings)\n",
    "skl_output = vectorizer.transform(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names()) #matched the vocab in custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vectorizer.idf_)  #matched the idf score of custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "skl_output.shape #matched the output shape to custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "with open('cleaned_strings (1)', 'rb') as f:\n",
    "    corpus = pickle.load(f)    #loading pickle file provided for assignment\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_task2(strings):              #func for fit()\n",
    "    req_of_word=[]\n",
    "    words=set()    #creating an empty set \n",
    "    count={}\n",
    "    if isinstance(strings,(list)): \n",
    "        for row in strings:\n",
    "            split_text=row.split(\" \")\n",
    "            for word in split_text:\n",
    "                if word in count:\n",
    "                    count[word]=count[word]+1        #count words in corpus\n",
    "                else:\n",
    "                    count[word]=1    \n",
    "        count=dict(sorted(count.items()))       #sort the dict\n",
    "        for i in strings:\n",
    "            for j in i.split(\" \"):    #split the strings acc to spaces\n",
    "                if len(j)<2:\n",
    "                    continue\n",
    "                words.add(j)             #append word length greater than 2 to the set\n",
    "        words=sorted(list(words))      #convert set into list and sort them\n",
    "        vocab={}                      #empty dictionary\n",
    "        for val,key in enumerate(words):\n",
    "            vocab[key]=val            #add splitted word as key and index as value in dict\n",
    "              \n",
    "            \n",
    "        idf_values_of_all_unique_words=idf(strings,vocab)\n",
    "        top50_idf_value_indices = np.argsort(list(idf_values_of_all_unique_words.values()))[::-1][:50]\n",
    "\n",
    "        top50_idf = np.take(list(idf_values_of_all_unique_words.values()), top50_idf_value_indices)\n",
    "        top_50_idf_words = np.take(list(idf_values_of_all_unique_words.keys()), top50_idf_value_indices)\n",
    "\n",
    "        reduced_idf = dict(zip(top_50_idf_words, top50_idf))\n",
    "        reduced_vocab = {key:value for value,key in enumerate(list(reduced_idf.keys()))}\n",
    "\n",
    "        return reduced_vocab, reduced_idf\n",
    "          #return vocabulary and unique idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vocab,reduced_idf=fit_task2(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zombiez': 0, 'havilland': 1, 'hearts': 2, 'heads': 3, 'hbo': 4, 'hayworth': 5, 'hayao': 6, 'hay': 7, 'hatred': 8, 'heche': 9, 'harris': 10, 'happy': 11, 'happiness': 12, 'hanks': 13, 'hankies': 14, 'hang': 15, 'heartwarming': 16, 'heels': 17, 'gone': 18, 'hero': 19, 'higher': 20, 'hide': 21, 'hes': 22, 'heroism': 23, 'heroine': 24, 'heroes': 25, 'hernandez': 26, 'heist': 27, 'hendrikson': 28, 'helping': 29, 'help': 30, 'helms': 31, 'hellish': 32, 'helen': 33, 'handles': 34, 'handle': 35, 'ham': 36, 'grade': 37, 'grates': 38, 'grasp': 39, 'graphics': 40, 'granted': 41, 'grainy': 42, 'gradually': 43, 'government': 44, 'halfway': 45, 'gotten': 46, 'gotta': 47, 'goth': 48, 'gosh': 49}\n"
     ]
    }
   ],
   "source": [
    "print(reduced_vocab) #reduced vocab of unique words for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zombiez': 6.922918004572872, 'havilland': 6.922918004572872, 'hearts': 6.922918004572872, 'heads': 6.922918004572872, 'hbo': 6.922918004572872, 'hayworth': 6.922918004572872, 'hayao': 6.922918004572872, 'hay': 6.922918004572872, 'hatred': 6.922918004572872, 'heche': 6.922918004572872, 'harris': 6.922918004572872, 'happy': 6.922918004572872, 'happiness': 6.922918004572872, 'hanks': 6.922918004572872, 'hankies': 6.922918004572872, 'hang': 6.922918004572872, 'heartwarming': 6.922918004572872, 'heels': 6.922918004572872, 'gone': 6.922918004572872, 'hero': 6.922918004572872, 'higher': 6.922918004572872, 'hide': 6.922918004572872, 'hes': 6.922918004572872, 'heroism': 6.922918004572872, 'heroine': 6.922918004572872, 'heroes': 6.922918004572872, 'hernandez': 6.922918004572872, 'heist': 6.922918004572872, 'hendrikson': 6.922918004572872, 'helping': 6.922918004572872, 'help': 6.922918004572872, 'helms': 6.922918004572872, 'hellish': 6.922918004572872, 'helen': 6.922918004572872, 'handles': 6.922918004572872, 'handle': 6.922918004572872, 'ham': 6.922918004572872, 'grade': 6.922918004572872, 'grates': 6.922918004572872, 'grasp': 6.922918004572872, 'graphics': 6.922918004572872, 'granted': 6.922918004572872, 'grainy': 6.922918004572872, 'gradually': 6.922918004572872, 'government': 6.922918004572872, 'halfway': 6.922918004572872, 'gotten': 6.922918004572872, 'gotta': 6.922918004572872, 'goth': 6.922918004572872, 'gosh': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "print(reduced_idf) #reduced idf of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek V\\Anaconda2020\\lib\\site-packages\\scipy\\sparse\\_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sparse_mat_task2=transform(corpus,reduced_vocab, reduced_idf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (19, 9)\t0.25819888974716115\n",
      "  (19, 13)\t0.25819888974716115\n",
      "  (19, 17)\t0.25819888974716115\n",
      "  (19, 30)\t0.25819888974716115\n",
      "  (19, 31)\t0.25819888974716115\n",
      "  (19, 38)\t0.25819888974716115\n",
      "  (19, 40)\t0.7745966692414833\n",
      "  (94, 49)\t1.0\n",
      "  (101, 18)\t1.0\n",
      "  (104, 35)\t1.0\n",
      "  (109, 0)\t0.7071067811865476\n",
      "  (109, 32)\t0.7071067811865476\n",
      "  (132, 25)\t1.0\n",
      "  (135, 12)\t0.5773502691896258\n",
      "  (135, 43)\t0.5773502691896258\n",
      "  (135, 46)\t0.5773502691896258\n",
      "  (180, 20)\t1.0\n",
      "  (191, 7)\t0.7071067811865475\n",
      "  (191, 24)\t0.7071067811865475\n",
      "  (197, 5)\t1.0\n",
      "  (222, 22)\t1.0\n",
      "  (225, 8)\t1.0\n",
      "  (232, 21)\t1.0\n",
      "  (234, 45)\t1.0\n",
      "  (236, 28)\t1.0\n",
      "  (253, 15)\t1.0\n",
      "  (270, 27)\t1.0\n",
      "  (277, 48)\t1.0\n",
      "  (323, 37)\t1.0\n",
      "  (343, 41)\t1.0\n",
      "  (371, 47)\t1.0\n",
      "  (421, 23)\t1.0\n",
      "  (430, 1)\t1.0\n",
      "  (437, 19)\t1.0\n",
      "  (459, 33)\t1.0\n",
      "  (462, 29)\t1.0\n",
      "  (475, 14)\t1.0\n",
      "  (532, 34)\t1.0\n",
      "  (533, 11)\t1.0\n",
      "  (539, 36)\t1.0\n",
      "  (572, 10)\t1.0\n",
      "  (610, 42)\t1.0\n",
      "  (625, 3)\t1.0\n",
      "  (628, 26)\t1.0\n",
      "  (633, 39)\t1.0\n",
      "  (644, 16)\t1.0\n",
      "  (660, 4)\t1.0\n",
      "  (681, 2)\t1.0\n",
      "  (703, 44)\t1.0\n",
      "  (714, 6)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25819889 0.         0.\n",
      "  0.         0.25819889 0.         0.         0.         0.25819889\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.\n",
      "  0.         0.         0.25819889 0.         0.77459667 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat_task2[19].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
